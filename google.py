# -*- coding: utf-8 -*-
"""google.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tJ1bhRSSa_UDwcLCSgmJlx84H1dnyB1p
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.init as init
import torch.utils.data as data
import torchvision.datasets as dset
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.autograd import Variable
import matplotlib.pyplot as plt
from torchvision.transforms import ToPILImage

batch_size= 1
learning_rate = 0.0002
num_epoch = 200

# from google.colab import files

# uploaded = files.upload()

# for fn in uploaded.keys():
#   print('User uploaded file "{name}" with length {length} bytes'.format(
#       name=fn, length=len(uploaded[fn])))

#!ls

#import os
#os.getcwd()

from google.colab import drive
drive.mount('/gdrive')

# 라벨(혹은 클래스) 별로 폴더가 저장되어 있는 루트 디렉토리를 지정
img_dir = "/gdrive"

# 해당 루트 디렉토리를 ImageFolder 함수에 전달함
# 이때 이미지들에 대한 변형도 같이 전달해줌
img_data = dset.ImageFolder(img_dir, transforms.Compose([
                                      transforms.Resize(256),                   # 이미지 크기를 256x256으로 바꿔줌
                                      transforms.RandomResizedCrop(224),        # 256x256 이미지의 랜덤한 위치에서 224x224 크기만큼 샘플링
                                      transforms.RandomHorizontalFlip(),        # 랜덤한 확률로 이미지를 좌우반전
                                      transforms.ToTensor()                    # 이미지 데이터를 텐서로 변형
            ]))

train_loader = data.DataLoader(img_data, batch_size=batch_size, shuffle=True, num_workers=2)
#test_loader= data.DataLoader(img_data, batch_size=batch_size, shuffle= False, num_workers=2)

# import matplotlib.pyplot as plt
# from torchvision.transforms import ToPILImage
to_img = ToPILImage()

#j,[image,label] in enumerate(train_loader):
for i, [data,label] in enumerate(train_loader):
    img = data[0][0,:]
    print(img.size())
 #   print("max: {}, min: {}".format(np.max(img.numpy()), np.min(img.numpy())))
    str_title = 'Label: '+str(label[0])
    plt.title(str_title)
    plt.imshow(to_img(img))

    plt.show()

def ComputeAccr(dloader, imodel):
    correct= 0
    total= 0
    
    for j, [imgs, labels] in enumerate(dloader): # batch_size 만큼
        img= Variable(imgs, volatile= True).cuda()   # x
        label= Variable(labels).cuda() # y
        
        output= imodel.forward(img) # forward prop.
        _, output_index= torch.max(output, 1)
        
        total += label.size(0)
        correct += (output_index == label).sum().float()
    print ("Accuracy of Test Data: {}".format(100*correct/total))

def conv_1(in_dim,out_dim):
    model = nn.Sequential(
        nn.Conv2d(in_dim,out_dim,1,1),
        nn.ReLU()
    )
    return model

def conv_1_3(in_dim,mid_dim,out_dim):
    model = nn.Sequential(
        nn.Conv2d(in_dim,mid_dim,1,1),
        nn.ReLU(),
        nn.Conv2d(mid_dim,out_dim,3,1,1),
        nn.ReLU()
    )
    return model
    
def conv_1_5(in_dim,mid_dim,out_dim):
    model = nn.Sequential(
        nn.Conv2d(in_dim,mid_dim,1,1),
        nn.ReLU(),
        nn.Conv2d(mid_dim,out_dim,5,1,2),
        nn.ReLU()
    )
    return model

def max_3_1(in_dim,out_dim):
    model = nn.Sequential(
        nn.MaxPool2d(3,1,1),
        nn.Conv2d(in_dim,out_dim,1,1),
        nn.ReLU()
    )
    return model

class inception_module(nn.Module):
    def __init__(self,in_dim,out_dim_1,mid_dim_3,out_dim_3,mid_dim_5,out_dim_5,pool):
        super(inception_module,self).__init__()
        self.conv_1 = conv_1(in_dim,out_dim_1)
        self.conv_1_3 = conv_1_3(in_dim,mid_dim_3,out_dim_3)
        self.conv_1_5 = conv_1_5(in_dim,mid_dim_5,out_dim_5)
        self.max_3_1 = max_3_1(in_dim,pool)

    def forward(self,x):
        out_1 = self.conv_1(x)
        out_2 = self.conv_1_3(x)
        out_3 = self.conv_1_5(x)
        out_4 = self.max_3_1(x)
        output = torch.cat([out_1,out_2,out_3,out_4],1)
        return output

class GoogLeNet(nn.Module):
    def __init__(self, base_dim,num_classes=2):
        super(GoogLeNet, self).__init__()
        self.num_classes=num_classes
        self.layer_1 = nn.Sequential(
            nn.Conv2d(3,base_dim,7,2,3),
            nn.MaxPool2d(3,2,1),
            nn.Conv2d(base_dim,base_dim*3,3,1,1),
            nn.MaxPool2d(3,2,1)
        )
        self.layer_2 = nn.Sequential(
            inception_module(base_dim*3,64,96,128,16,32,32),
            inception_module(base_dim*4,128,128,192,32,96,64),
            nn.MaxPool2d(3,2,1)
        )
        self.layer_3 = nn.Sequential(
            inception_module(480,192,96,208,16,48,64),
            inception_module(512,160,112,224,24,64,64),
            inception_module(512,128,128,256,24,64,64),
            inception_module(512,112,144,288,32,64,64),
            inception_module(528,256,160,320,32,128,128),
            nn.MaxPool2d(3,2,1)
        )
        self.layer_4 = nn.Sequential(
            inception_module(832,256,160,320,32,128,128),
            inception_module(832,384,192,384,48,128,128), 
            nn.AvgPool2d(7,1)
        )
        self.layer_5 = nn.Dropout2d(0.4)
        self.fc_layer = nn.Linear(1024,self.num_classes)
                
        
    def forward(self, x):
        out = self.layer_1(x)
        out = self.layer_2(out)
        out = self.layer_3(out)
        out = self.layer_4(out)
        out = self.layer_5(out)
        out = out.view(batch_size,-1) 
        out = self.fc_layer(out)
        return out
model = GoogLeNet(base_dim=64)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

model = GoogLeNet(base_dim=64)
for i in model.named_children():
    print(i)

model = GoogLeNet(base_dim=64).to(device)
loss_func = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(),lr=learning_rate)

model.train()

acc_list2 = []
total_loss = 0

for i in range(num_epoch):
    for j,[image,label] in enumerate(train_loader):
        x = image.to(device)
        y_= label.to(device)
        
        optimizer.zero_grad()
        output = model.forward(x)
        loss = loss_func(output,y_)
        loss.backward()
        optimizer.step()
        total_loss = total_loss + loss.item()
        
    if i % 10 == 0:
        print(loss)
        acc_list2.append(total_loss)
        total_loss = 0

model.eval()  
ComputeAccr(test_loader, model)

